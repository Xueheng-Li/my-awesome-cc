#!/usr/bin/env python3
"""
GitHub Trending çˆ¬è™«è„šæœ¬
è·å– GitHub çƒ­é—¨é¡¹ç›®å’Œä»“åº“ README
"""

import requests
from bs4 import BeautifulSoup
import re
import sys
import argparse
from datetime import datetime


def get_github_trending(since: str = "daily", language: str = "") -> str:
    """è·å– GitHub trending æ¦œå•"""
    url = f"https://github.com/trending/{language.lower()}?since={since}" if language else f"https://github.com/trending?since={since}"

    try:
        current_date = datetime.now()
        date_str = current_date.strftime("%Yå¹´%mæœˆ%dæ—¥")
        weekday_names = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"]
        weekday_str = weekday_names[current_date.weekday()]
        since_display = {"daily": "ä»Šæ—¥", "weekly": "æœ¬å‘¨", "monthly": "æœ¬æœˆ"}

        response = requests.get(url, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')
        articles = soup.find_all('article', class_='Box-row')

        if not articles:
            return f"âŒ æœªæ‰¾åˆ° trending é¡¹ç›®\nè¯·æ±‚URL: {url}"

        result = []
        result.append(f"ğŸŒŸ GitHub Trending Repositories")
        result.append(f"ğŸ“… è·å–æ—¶é—´: {date_str} {weekday_str}")
        result.append(f"â° æ—¶é—´èŒƒå›´: {since_display.get(since, since)}")
        if language:
            result.append(f"ğŸ’» ç¼–ç¨‹è¯­è¨€: {language}")
        result.append(f"ğŸ“Š å…±å‘ç° {len(articles)} ä¸ªçƒ­é—¨é¡¹ç›®")
        result.append("")

        for i, article in enumerate(articles, 1):
            try:
                title_elem = article.find('h2', class_='h3')
                if not title_elem:
                    continue
                title_link = title_elem.find('a')
                if not title_link:
                    continue

                title = ' '.join(title_link.get_text(strip=True).split())
                project_url = "https://github.com" + title_link.get('href', '')

                description_elem = article.find('p', class_='col-9')
                description = description_elem.get_text(strip=True) if description_elem else "æ— æè¿°"

                language_elem = article.find('span', {'itemprop': 'programmingLanguage'})
                project_language = language_elem.get_text(strip=True) if language_elem else "æœªçŸ¥"

                star_link = article.find('a', href=re.compile(r'/stargazers'))
                total_stars = star_link.get_text(strip=True) if star_link else "0"

                fork_link = article.find('a', href=re.compile(r'/forks'))
                total_forks = fork_link.get_text(strip=True) if fork_link else "0"

                period_stars = "0"
                for span in article.find_all('span'):
                    span_text = span.get_text(strip=True)
                    if 'stars' in span_text.lower() and ('today' in span_text.lower() or 'this week' in span_text.lower() or 'this month' in span_text.lower()):
                        period_match = re.search(r'(\d+[,\d]*)\s*stars?', span_text, re.IGNORECASE)
                        if period_match:
                            period_stars = period_match.group(1)
                        break

                result.append(f"{i}. {title}")
                result.append(f"   ğŸ”— {project_url}")
                result.append(f"   ğŸ“ {description}")
                result.append(f"   ğŸ’» è¯­è¨€: {project_language} | â­ æ€»æ˜Ÿæ•°: {total_stars} | ğŸ´ Forks: {total_forks} | ğŸ”¥ {since_display.get(since, since)}: +{period_stars}")

            except Exception as e:
                result.append(f"âŒ è§£æç¬¬ {i} ä¸ªé¡¹ç›®æ—¶å‡ºé”™: {str(e)}")
                continue

        return "\n".join(result)

    except requests.exceptions.RequestException as e:
        return f"âŒ ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)}\nè¯·æ±‚URL: {url}"
    except Exception as e:
        return f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {str(e)}"


def get_repository_readme(repo: str) -> str:
    """è·å–æŒ‡å®šä»“åº“çš„ README"""
    repo = repo.strip()
    branches = ['main', 'master']
    readme_files = ['README.md', 'readme.md', 'Readme.md', 'README.txt', 'readme.txt']

    for branch in branches:
        for readme_file in readme_files:
            url = f"https://raw.githubusercontent.com/{repo}/refs/heads/{branch}/{readme_file}"
            try:
                response = requests.get(url, timeout=20)
                if response.status_code == 200:
                    content = response.text
                    if len(content) > 50000:
                        content = content[:50000] + "\n\n... [å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­] ..."
                    return f"âœ… æˆåŠŸè·å– {repo} çš„ README\næ¥æº: {url}\n\n{content}"
            except:
                continue

    return f"âŒ æœªæ‰¾åˆ° {repo} çš„ README æ–‡ä»¶"


def main():
    parser = argparse.ArgumentParser(description='GitHub Trending å·¥å…·')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # trending å­å‘½ä»¤
    trending_parser = subparsers.add_parser('trending', help='è·å– trending åˆ—è¡¨')
    trending_parser.add_argument('--since', '-s', default='daily', choices=['daily', 'weekly', 'monthly'], help='æ—¶é—´èŒƒå›´')
    trending_parser.add_argument('--language', '-l', default='', help='ç¼–ç¨‹è¯­è¨€è¿‡æ»¤')

    # readme å­å‘½ä»¤
    readme_parser = subparsers.add_parser('readme', help='è·å–ä»“åº“ README')
    readme_parser.add_argument('repo', help='ä»“åº“åç§° (owner/repo)')

    args = parser.parse_args()

    if args.command == 'trending':
        print(get_github_trending(args.since, args.language))
    elif args.command == 'readme':
        print(get_repository_readme(args.repo))
    else:
        # é»˜è®¤è¡Œä¸ºï¼šè·å– daily trending
        print(get_github_trending())


if __name__ == "__main__":
    main()
